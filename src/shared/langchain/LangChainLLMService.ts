import { BaseChatModel } from "@langchain/core/language_models/chat_models";
import { ChatPromptTemplate } from "@langchain/core/prompts";
import {
  BaseMessage,
  HumanMessage,
  SystemMessage
} from "@langchain/core/messages";

import { LangChainModelFactory } from './LangChainModelFactory';
import { ModelConfigEntity } from '../entities/ModelConfigEntity';

/**
 * LangChain LLMæœåŠ¡
 * æä¾›ç»Ÿä¸€çš„LLMè°ƒç”¨æ¥å£ï¼Œå†…éƒ¨ä½¿ç”¨LangChainæ¨¡å‹
 */
export class LangChainLLMService {
  private modelCache: Map<string, BaseChatModel> = new Map();
  private configCache: Map<string, ModelConfigEntity> = new Map();

  /**
   * å‘é€æ¶ˆæ¯
   * @param message ç”¨æˆ·æ¶ˆæ¯
   * @param configId æ¨¡å‹é…ç½®ID
   * @param systemPrompt å¯é€‰çš„ç³»ç»Ÿæç¤ºè¯
   * @returns æ¨¡å‹å“åº”
   */
  async sendMessage(
    message: string,
    configId: string,
    systemPrompt?: string
  ): Promise<string> {
    const model = await this.getModel(configId);

    const messages: BaseMessage[] = [
      ...(systemPrompt ? [new SystemMessage(systemPrompt)] : []),
      new HumanMessage(message)
    ];

    const response = await model.invoke(messages);
    return response.content as string;
  }

  /**
   * ä½¿ç”¨ä¸´æ—¶é…ç½®å‘é€æ¶ˆæ¯ï¼ˆä¸ç¼“å­˜æ¨¡å‹ï¼‰
   * @param message ç”¨æˆ·æ¶ˆæ¯
   * @param config ä¸´æ—¶æ¨¡å‹é…ç½®
   * @param systemPrompt å¯é€‰çš„ç³»ç»Ÿæç¤ºè¯
   * @returns æ¨¡å‹å“åº”
   */
  async sendMessageWithConfig(
    message: string,
    config: ModelConfigEntity,
    systemPrompt?: string
  ): Promise<string> {
    // ç›´æ¥åˆ›å»ºæ¨¡å‹ï¼Œä¸ä½¿ç”¨ç¼“å­˜
    const model = LangChainModelFactory.createChatModel(config);

    const messages: BaseMessage[] = [
      ...(systemPrompt ? [new SystemMessage(systemPrompt)] : []),
      new HumanMessage(message)
    ];

    const response = await model.invoke(messages);
    return response.content as string;
  }

  /**
   * è·å–æä¾›å•†çš„å¯ç”¨æ¨¡å‹åˆ—è¡¨
   * @param config æ¨¡å‹é…ç½®ï¼ˆç”¨äºè·å–APIå¯†é’¥ç­‰ä¿¡æ¯ï¼‰
   * @returns å¯ç”¨æ¨¡å‹åˆ—è¡¨
   */
  async getAvailableModels(config: ModelConfigEntity): Promise<string[]> {
    const provider = config.provider.toLowerCase();

    switch (provider) {
      case 'gemini':
      case 'google':
        return await this.getGeminiModels(config);

      case 'openai':
        return await this.getOpenAIModels(config);

      case 'claude':
      case 'anthropic':
        // Claudeé€šå¸¸ä¸æä¾›æ¨¡å‹åˆ—è¡¨APIï¼Œè¿”å›é¢„å®šä¹‰åˆ—è¡¨
        return [
          'claude-3-5-sonnet-20241022',
          'claude-3-5-haiku-20241022',
          'claude-3-opus-20240229',
          'claude-3-sonnet-20240229',
          'claude-3-haiku-20240307'
        ];

      case 'custom':
        // è‡ªå®šä¹‰æä¾›å•†ï¼Œå°è¯•é€šè¿‡APIè·å–æ¨¡å‹åˆ—è¡¨
        return await this.getCustomProviderModels(config);

      default:
        throw new Error(`ä¸æ”¯æŒçš„æä¾›å•†: ${provider}`);
    }
  }

  /**
   * ä½¿ç”¨æç¤ºè¯æ¨¡æ¿å‘é€æ¶ˆæ¯
   * @param template æç¤ºè¯æ¨¡æ¿
   * @param variables æ¨¡æ¿å˜é‡
   * @param configId æ¨¡å‹é…ç½®ID
   * @returns æ¨¡å‹å“åº”
   */
  async sendMessageWithTemplate(
    template: ChatPromptTemplate,
    variables: Record<string, any>,
    configId: string
  ): Promise<string> {
    const model = await this.getModel(configId);
    const chain = template.pipe(model);
    const response = await chain.invoke(variables);
    return response.content as string;
  }

  /**
   * å‘é€å¤šè½®å¯¹è¯æ¶ˆæ¯
   * @param messages æ¶ˆæ¯å†å²
   * @param configId æ¨¡å‹é…ç½®ID
   * @returns æ¨¡å‹å“åº”
   */
  async sendConversation(
    messages: BaseMessage[],
    configId: string
  ): Promise<string> {
    const model = await this.getModel(configId);
    const response = await model.invoke(messages);
    return response.content as string;
  }

  /**
   * æµå¼å‘é€æ¶ˆæ¯
   * @param message ç”¨æˆ·æ¶ˆæ¯
   * @param configId æ¨¡å‹é…ç½®ID
   * @param systemPrompt å¯é€‰çš„ç³»ç»Ÿæç¤ºè¯
   * @param onChunk å¤„ç†æ¯ä¸ªchunkçš„å›è°ƒå‡½æ•°
   */
  async streamMessage(
    message: string,
    configId: string,
    systemPrompt?: string,
    onChunk?: (chunk: string) => void
  ): Promise<string> {
    const model = await this.getModel(configId);
    
    const messages: BaseMessage[] = [
      ...(systemPrompt ? [new SystemMessage(systemPrompt)] : []),
      new HumanMessage(message)
    ];
    
    let fullResponse = '';
    const stream = await model.stream(messages);
    
    for await (const chunk of stream) {
      const content = chunk.content as string;
      fullResponse += content;
      if (onChunk) {
        onChunk(content);
      }
    }
    
    return fullResponse;
  }

  /**
   * æ‰¹é‡å¤„ç†æ¶ˆæ¯
   * @param requests æ‰¹é‡è¯·æ±‚
   * @param configId æ¨¡å‹é…ç½®ID
   * @returns æ‰¹é‡å“åº”
   */
  async batchMessages(
    requests: Array<{
      message: string;
      systemPrompt?: string;
    }>,
    configId: string
  ): Promise<string[]> {
    const model = await this.getModel(configId);
    
    const messageBatches = requests.map(req => [
      ...(req.systemPrompt ? [new SystemMessage(req.systemPrompt)] : []),
      new HumanMessage(req.message)
    ]);
    
    const responses = await model.batch(messageBatches);
    return responses.map(response => response.content as string);
  }

  /**
   * æµ‹è¯•æ¨¡å‹è¿æ¥
   * @param configId æ¨¡å‹é…ç½®ID
   * @returns æµ‹è¯•ç»“æœ
   */
  async testModel(configId: string): Promise<{ success: boolean; error?: string; responseTime?: number }> {
    try {
      const startTime = Date.now();
      await this.sendMessage('Hello', configId);
      const responseTime = Date.now() - startTime;
      
      return {
        success: true,
        responseTime
      };
    } catch (error) {
      return {
        success: false,
        error: error instanceof Error ? error.message : 'æœªçŸ¥é”™è¯¯'
      };
    }
  }

  /**
   * è·å–æˆ–åˆ›å»ºæ¨¡å‹å®ä¾‹
   * @param configId é…ç½®ID
   * @returns æ¨¡å‹å®ä¾‹
   */
  private async getModel(configId: string): Promise<BaseChatModel> {
    if (!this.modelCache.has(configId)) {
      const config = await this.getConfig(configId);
      const model = LangChainModelFactory.createChatModel(config);
      this.modelCache.set(configId, model);
    }
    return this.modelCache.get(configId)!;
  }

  /**
   * è·å–é…ç½®ï¼ˆè¿™é‡Œéœ€è¦å®ç°é…ç½®è·å–é€»è¾‘ï¼‰
   * @param configId é…ç½®ID
   * @returns é…ç½®å®ä½“
   */
  private async getConfig(configId: string): Promise<ModelConfigEntity> {
    if (!this.configCache.has(configId)) {
      // TODO: è¿™é‡Œéœ€è¦å®ç°ä»æ•°æ®åº“æˆ–é…ç½®å­˜å‚¨ä¸­è·å–é…ç½®çš„é€»è¾‘
      // æš‚æ—¶æŠ›å‡ºé”™è¯¯ï¼Œæé†’éœ€è¦å®ç°
      throw new Error(`é…ç½®è·å–æœªå®ç°: ${configId}`);
    }
    return this.configCache.get(configId)!;
  }

  /**
   * è®¾ç½®é…ç½®ï¼ˆç”¨äºæµ‹è¯•æˆ–æ‰‹åŠ¨è®¾ç½®ï¼‰
   * @param configId é…ç½®ID
   * @param config é…ç½®å®ä½“
   */
  setConfig(configId: string, config: ModelConfigEntity): void {
    this.configCache.set(configId, config);
    // æ¸…é™¤å¯¹åº”çš„æ¨¡å‹ç¼“å­˜ï¼Œå¼ºåˆ¶é‡æ–°åˆ›å»º
    this.modelCache.delete(configId);
  }

  /**
   * æ¸…é™¤ç¼“å­˜
   * @param configId å¯é€‰çš„é…ç½®IDï¼Œå¦‚æœä¸æä¾›åˆ™æ¸…é™¤æ‰€æœ‰ç¼“å­˜
   */
  clearCache(configId?: string): void {
    if (configId) {
      this.modelCache.delete(configId);
      this.configCache.delete(configId);
    } else {
      this.modelCache.clear();
      this.configCache.clear();
    }
  }

  /**
   * è·å–Geminiå¯ç”¨æ¨¡å‹åˆ—è¡¨
   * @param config æ¨¡å‹é…ç½®
   * @returns Geminiæ¨¡å‹åˆ—è¡¨
   */
  private async getGeminiModels(config: ModelConfigEntity): Promise<string[]> {
    try {
      const baseURL = config.baseURL || 'https://generativelanguage.googleapis.com';
      const url = `${baseURL}/v1beta/models?key=${config.apiKey}`;

      console.log(`ğŸ” è·å–Geminiæ¨¡å‹åˆ—è¡¨: ${url.replace(config.apiKey, '***')}`);

      const response = await fetch(url);

      if (!response.ok) {
        const errorText = await response.text();
        console.error(`âŒ Geminiæ¨¡å‹åˆ—è¡¨è·å–å¤±è´¥ (${response.status}): ${errorText}`);

        // æŠ›å‡ºè¯¦ç»†é”™è¯¯è€Œä¸æ˜¯é™çº§ï¼Œè®©ç”¨æˆ·çŸ¥é“å…·ä½“é—®é¢˜
        throw new Error(`Gemini APIè°ƒç”¨å¤±è´¥ (${response.status}): ${errorText.substring(0, 200)}`);
      }

      const data = await response.json();
      console.log('ğŸ” Gemini APIåŸå§‹å“åº”:', JSON.stringify(data, null, 2));

      const models = this.parseGeminiModelsResponse(data);

      if (models.length === 0) {
        console.error('âŒ è§£æåçš„Geminiæ¨¡å‹åˆ—è¡¨ä¸ºç©º');
        console.log('åŸå§‹æ•°æ®ç»“æ„:', data);
        throw new Error('Gemini APIè¿”å›äº†æ•°æ®ä½†è§£æåæ¨¡å‹åˆ—è¡¨ä¸ºç©ºï¼Œå¯èƒ½æ˜¯APIæ ¼å¼å˜åŒ–');
      }

      console.log(`âœ… æˆåŠŸè·å– ${models.length} ä¸ªGeminiæ¨¡å‹:`, models);
      return models;

    } catch (error) {
      console.error('âŒ Geminiæ¨¡å‹åˆ—è¡¨è·å–å¼‚å¸¸:', error);
      // ä¸å†è‡ªåŠ¨é™çº§ï¼ŒæŠ›å‡ºé”™è¯¯è®©ç”¨æˆ·çŸ¥é“å…·ä½“é—®é¢˜
      throw error instanceof Error ? error : new Error(`Geminiæ¨¡å‹è·å–å¤±è´¥: ${error}`);
    }
  }

  /**
   * è·å–OpenAIå¯ç”¨æ¨¡å‹åˆ—è¡¨
   * @param config æ¨¡å‹é…ç½®
   * @returns OpenAIæ¨¡å‹åˆ—è¡¨
   */
  private async getOpenAIModels(config: ModelConfigEntity): Promise<string[]> {
    try {
      const baseURL = config.baseURL || 'https://api.openai.com/v1';
      const url = `${baseURL}/models`;

      const response = await fetch(url, {
        headers: {
          'Authorization': `Bearer ${config.apiKey}`
        }
      });

      if (!response.ok) {
        console.warn(`OpenAIæ¨¡å‹åˆ—è¡¨è·å–å¤±è´¥ (${response.status})ï¼Œä½¿ç”¨é¢„å®šä¹‰åˆ—è¡¨`);
        return this.getDefaultOpenAIModels();
      }

      const data = await response.json();
      const models = this.parseOpenAIModelsResponse(data);

      if (models.length === 0) {
        console.warn('æœªè·å–åˆ°OpenAIæ¨¡å‹ï¼Œä½¿ç”¨é¢„å®šä¹‰åˆ—è¡¨');
        return this.getDefaultOpenAIModels();
      }

      console.log(`âœ… æˆåŠŸè·å– ${models.length} ä¸ªOpenAIæ¨¡å‹`);
      return models;

    } catch (error) {
      console.warn('OpenAIæ¨¡å‹åˆ—è¡¨è·å–å¼‚å¸¸ï¼Œä½¿ç”¨é¢„å®šä¹‰åˆ—è¡¨:', error);
      return this.getDefaultOpenAIModels();
    }
  }

  /**
   * è§£æGeminiæ¨¡å‹å“åº”
   * @param data APIå“åº”æ•°æ®
   * @returns æ¨¡å‹åç§°åˆ—è¡¨
   */
  private parseGeminiModelsResponse(data: any): string[] {
    try {
      if (data.models && Array.isArray(data.models)) {
        const models = data.models
          .map((model: any) => {
            // Gemini APIè¿”å›æ ¼å¼: "models/gemini-1.5-pro"
            const modelName = model.name || model.id;
            if (typeof modelName === 'string' && modelName.startsWith('models/')) {
              return {
                name: modelName.replace('models/', ''),
                displayName: model.displayName || modelName.replace('models/', ''),
                description: model.description || '',
                inputTokenLimit: model.inputTokenLimit || 0,
                outputTokenLimit: model.outputTokenLimit || 0,
                supportedMethods: model.supportedGenerationMethods || []
              };
            }
            return null;
          })
          .filter((model: any) => {
            // è¿‡æ»¤å‡ºæœ‰æ•ˆçš„GeminièŠå¤©æ¨¡å‹
            if (!model || !model.name) {
              console.log('âŒ è·³è¿‡æ— æ•ˆæ¨¡å‹:', model);
              return false;
            }

            const name = model.name.toLowerCase();
            const isGeminiModel = name.includes('gemini') || name.includes('bison') || name.includes('chat-bison');
            const isChatModel = model.supportedMethods.includes('generateContent') ||
                               name.includes('chat') ||
                               name.includes('pro') ||
                               name.includes('flash') ||
                               name.includes('gemini'); // æ”¾å®½æ¡ä»¶ï¼šåŒ…å«geminiçš„éƒ½è®¤ä¸ºæ˜¯èŠå¤©æ¨¡å‹
            const isNotEmbedding = !name.includes('embedding') &&
                                  !name.includes('aqa') &&
                                  !name.includes('text-bison') &&
                                  !name.includes('imagen');

            const shouldInclude = isGeminiModel && isChatModel && isNotEmbedding;

            console.log(`ğŸ” æ¨¡å‹è¿‡æ»¤: ${model.name}`, {
              isGeminiModel,
              isChatModel,
              isNotEmbedding,
              shouldInclude,
              supportedMethods: model.supportedMethods
            });

            return shouldInclude;
          })
          .sort((a: any, b: any) => {
            // æ’åºï¼š2.5ç‰ˆæœ¬ > 2.0ç‰ˆæœ¬ > 1.5ç‰ˆæœ¬ > 1.0ç‰ˆæœ¬ï¼ŒPro > Flash > å…¶ä»–
            const getVersionScore = (name: string) => {
              if (name.includes('2.5')) return 250;
              if (name.includes('2.0')) return 200;
              if (name.includes('1.5')) return 150;
              if (name.includes('1.0')) return 100;
              return 50;
            };

            const getTypeScore = (name: string) => {
              if (name.includes('pro')) return 30;
              if (name.includes('flash')) return 20;
              return 10;
            };

            const scoreA = getVersionScore(a.name) + getTypeScore(a.name);
            const scoreB = getVersionScore(b.name) + getTypeScore(b.name);

            return scoreB - scoreA; // é™åºæ’åˆ—
          })
          .map((model: any) => model.name);

        console.log(`âœ… è§£æåˆ° ${models.length} ä¸ªGeminièŠå¤©æ¨¡å‹:`, models.slice(0, 5));
        return models;
      }
    } catch (error) {
      console.warn('è§£æGeminiæ¨¡å‹å“åº”å¤±è´¥:', error);
    }
    return [];
  }

  /**
   * è§£æOpenAIæ¨¡å‹å“åº”
   * @param data APIå“åº”æ•°æ®
   * @returns æ¨¡å‹åç§°åˆ—è¡¨
   */
  private parseOpenAIModelsResponse(data: any): string[] {
    try {
      if (data.data && Array.isArray(data.data)) {
        return data.data
          .map((model: any) => model.id)
          .filter((id: string) => {
            if (!id) return false;
            
            // æ’é™¤éèŠå¤©æ¨¡å‹
            const excludePatterns = [
              'text-embedding',    // embeddingæ¨¡å‹
              'tts-',             // æ–‡æœ¬è½¬è¯­éŸ³æ¨¡å‹
              'whisper',          // è¯­éŸ³è¯†åˆ«æ¨¡å‹
              'dall-e',           // å›¾åƒç”Ÿæˆæ¨¡å‹
              'davinci-002',      // æ—§ç‰ˆæœ¬æ¨¡å‹
              'text-ada',         // æ—§ç‰ˆæœ¬embedding
              'transcribe',       // è½¬å½•æ¨¡å‹
              'image-1'           // å›¾åƒå¤„ç†æ¨¡å‹
            ];
            
            // å¦‚æœåŒ…å«æ’é™¤æ¨¡å¼ï¼Œè·³è¿‡
            if (excludePatterns.some(pattern => id.includes(pattern))) {
              return false;
            }
            
            // åŒ…å«èŠå¤©æ¨¡å‹æ¨¡å¼
            const includePatterns = [
              'gpt',              // GPTç³»åˆ—
              'o1',               // O1ç³»åˆ—
              'o3',               // O3ç³»åˆ—
              'o4',               // O4ç³»åˆ—
              'chatgpt',          // ChatGPTç³»åˆ—
              'claude',           // Claudeç³»åˆ—
              'gemini',           // Geminiç³»åˆ—
              'deepseek',         // DeepSeekç³»åˆ—
              'grok',             // Grokç³»åˆ—
              'qwen',             // Qwenç³»åˆ—
              'kimi'              // Kimiç³»åˆ—
            ];
            
            // åŒ…å«ä»»æ„ä¸€ä¸ªæ¨¡å¼å³å¯
            return includePatterns.some(pattern => id.includes(pattern));
          })
          .sort();
      }
    } catch (error) {
      console.warn('è§£æOpenAIæ¨¡å‹å“åº”å¤±è´¥:', error);
    }
    return [];
  }



  /**
   * è·å–é»˜è®¤OpenAIæ¨¡å‹åˆ—è¡¨
   * @returns é¢„å®šä¹‰çš„OpenAIæ¨¡å‹åˆ—è¡¨
   */
  private getDefaultOpenAIModels(): string[] {
    return [
      'gpt-4o',
      'gpt-4o-mini',
      'gpt-4-turbo',
      'gpt-4',
      'gpt-3.5-turbo',
      'o1-preview',
      'o1-mini'
    ];
  }

  /**
   * è·å–è‡ªå®šä¹‰æä¾›å•†çš„æ¨¡å‹åˆ—è¡¨
   * @param config æ¨¡å‹é…ç½®
   * @returns å¯ç”¨æ¨¡å‹åˆ—è¡¨
   */
  private async getCustomProviderModels(config: ModelConfigEntity): Promise<string[]> {
    try {
      // å¯¹äºè‡ªå®šä¹‰æä¾›å•†ï¼Œå°è¯•é€šè¿‡é€šç”¨APIè·å–æ¨¡å‹åˆ—è¡¨
      // å¤§å¤šæ•°OpenAIå…¼å®¹çš„APIéƒ½æ”¯æŒ /v1/models ç«¯ç‚¹
      const baseURL = config.baseURL.replace(/\/+$/, ''); // ç§»é™¤æœ«å°¾çš„æ–œæ 
      const modelsURL = `${baseURL}/models`;

      console.log(`ğŸ” è·å–è‡ªå®šä¹‰æä¾›å•†æ¨¡å‹åˆ—è¡¨: ${modelsURL}`);

      const response = await fetch(modelsURL, {
        method: 'GET',
        headers: {
          'Authorization': `Bearer ${config.apiKey}`,
          'Content-Type': 'application/json'
        }
      });

      if (!response.ok) {
        console.warn(`è·å–è‡ªå®šä¹‰æä¾›å•†æ¨¡å‹åˆ—è¡¨å¤±è´¥: ${response.status} ${response.statusText}`);
        return this.getDefaultCustomModels();
      }

      const data = await response.json();
      console.log('ğŸ” è‡ªå®šä¹‰æä¾›å•†APIåŸå§‹å“åº”:', JSON.stringify(data, null, 2));

      if (data.data && Array.isArray(data.data)) {
        const models = data.data
          .map((model: any) => model.id)
          .filter((id: string) => id && typeof id === 'string')
          .sort();

        console.log(`âœ… æˆåŠŸè·å– ${models.length} ä¸ªè‡ªå®šä¹‰æä¾›å•†æ¨¡å‹:`, models);
        return models.length > 0 ? models : this.getDefaultCustomModels();
      }
    } catch (error) {
      console.warn('è·å–è‡ªå®šä¹‰æä¾›å•†æ¨¡å‹åˆ—è¡¨æ—¶å‡ºé”™:', error);
    }

    return this.getDefaultCustomModels();
  }

  /**
   * è·å–é»˜è®¤çš„è‡ªå®šä¹‰æ¨¡å‹åˆ—è¡¨
   * @returns é¢„å®šä¹‰çš„é€šç”¨æ¨¡å‹åˆ—è¡¨
   */
  private getDefaultCustomModels(): string[] {
    return [
      // Moonshot AI æ¨¡å‹
      'kimi-k2-0711-preview',
      'moonshot-v1-8k',
      'moonshot-v1-32k',
      'moonshot-v1-128k',
      // é€šç”¨OpenAIå…¼å®¹æ¨¡å‹
      'gpt-3.5-turbo',
      'gpt-4',
      'gpt-4-turbo',
      'gpt-4o',
      'gpt-4o-mini',
      // å…¶ä»–å¸¸è§ç¬¬ä¸‰æ–¹æ¨¡å‹
      'deepseek-chat',
      'deepseek-coder',
      'qwen-turbo',
      'qwen-plus',
      'chatglm-6b',
      'chatglm2-6b',
      'llama-2-7b-chat',
      'llama-2-13b-chat',
      'mistral-7b-instruct',
      'custom-model'
    ];
  }
}
