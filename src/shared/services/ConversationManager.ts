import { BaseMessage, HumanMessage, AIMessage, SystemMessage, trimMessages } from '@langchain/core/messages'
import { ChatMessage } from '../types'
import log from 'electron-log'

/**
 * å¯¹è¯ç®¡ç†å™¨
 * è´Ÿè´£ç®¡ç†èŠå¤©å†å²ã€ä¸Šä¸‹æ–‡çª—å£å’Œæ¶ˆæ¯æ ¼å¼è½¬æ¢
 */
export class ConversationManager {
  private static instance: ConversationManager
  
  // ä¸åŒæ¨¡å‹çš„ä¸Šä¸‹æ–‡çª—å£é™åˆ¶ï¼ˆtokenæ•°ï¼‰
  private readonly MODEL_CONTEXT_LIMITS: Record<string, number> = {
    'gpt-4o': 128000,
    'gpt-4o-mini': 128000,
    'gpt-4-turbo': 128000,
    'gpt-4': 8192,
    'gpt-3.5-turbo': 4096,
    'claude-3-5-sonnet-20241022': 200000,
    'claude-3-5-sonnet-20240620': 200000,
    'claude-3-opus-20240229': 200000,
    'claude-3-sonnet-20240229': 200000,
    'claude-3-haiku-20240307': 200000,
    'claude-sonnet-4-20250514': 200000,
    'gemini-2.0-flash-exp': 1000000,
    'gemini-1.5-pro': 2000000,
    'gemini-1.5-flash': 1000000,
  }

  private constructor() {}

  static getInstance(): ConversationManager {
    if (!ConversationManager.instance) {
      ConversationManager.instance = new ConversationManager()
    }
    return ConversationManager.instance
  }

  /**
   * å°†ChatMessageæ•°ç»„è½¬æ¢ä¸ºLangChain BaseMessageæ•°ç»„
   */
  convertToBaseMessages(messages: ChatMessage[]): BaseMessage[] {
    return messages.map(msg => {
      if (msg.role === 'user') {
        return new HumanMessage(msg.content)
      } else if (msg.role === 'system') {
        return new SystemMessage(msg.content)
      } else {
        return new AIMessage(msg.content)
      }
    })
  }

  /**
   * è·å–æ¨¡å‹çš„ä¸Šä¸‹æ–‡çª—å£é™åˆ¶
   */
  getModelContextLimit(modelName: string): number {
    // æå–æ¨¡å‹åç§°ï¼ˆå»é™¤é…ç½®IDå‰ç¼€ï¼‰
    const cleanModelName = this.extractModelName(modelName)
    
    // æŸ¥æ‰¾åŒ¹é…çš„æ¨¡å‹é™åˆ¶
    const limit = this.MODEL_CONTEXT_LIMITS[cleanModelName]
    if (limit) {
      return limit
    }

    // æ™ºèƒ½æ¨æ–­ï¼šæ ¹æ®æ¨¡å‹åç§°ç‰¹å¾åˆ¤æ–­
    const lowerName = cleanModelName.toLowerCase()
    
    if (lowerName.includes('claude')) {
      return 200000 // Claudeç³»åˆ—é»˜è®¤20ä¸‡token
    } else if (lowerName.includes('gpt-4o')) {
      return 128000 // GPT-4oç³»åˆ—é»˜è®¤12.8ä¸‡token
    } else if (lowerName.includes('gpt-4')) {
      return 8192   // GPT-4é»˜è®¤8k token
    } else if (lowerName.includes('gpt-3.5')) {
      return 4096   // GPT-3.5é»˜è®¤4k token
    } else if (lowerName.includes('gemini')) {
      return 1000000 // Geminié»˜è®¤100ä¸‡token
    } else {
      return 4096   // ä¿å®ˆé»˜è®¤å€¼
    }
  }

  /**
   * ä»æ¨¡å‹IDä¸­æå–çº¯æ¨¡å‹åç§°
   */
  private extractModelName(modelId: string): string {
    // å¦‚æœæ˜¯UUID-æ¨¡å‹åæ ¼å¼ï¼Œæå–ååŠéƒ¨åˆ†
    const parts = modelId.split('-')
    if (parts.length >= 6) {
      return parts.slice(5).join('-')
    }
    return modelId
  }

  /**
   * ç®€å•çš„tokenè®¡æ•°å™¨ï¼ˆåŸºäºå­—ç¬¦æ•°ä¼°ç®—ï¼‰
   * 1 token â‰ˆ 4ä¸ªå­—ç¬¦ï¼ˆè‹±æ–‡ï¼‰æˆ– 1.5ä¸ªå­—ç¬¦ï¼ˆä¸­æ–‡ï¼‰
   */
  private estimateTokenCount(messages: BaseMessage[]): number {
    const totalChars = messages.reduce((count, msg) => {
      const content = typeof msg.content === 'string' ? msg.content : JSON.stringify(msg.content)
      return count + content.length
    }, 0)

    // æ··åˆè¯­è¨€çš„ä¿å®ˆä¼°ç®—ï¼šå¹³å‡2.5ä¸ªå­—ç¬¦ = 1 token
    return Math.ceil(totalChars / 2.5)
  }

  /**
   * å‡†å¤‡å¯¹è¯ä¸Šä¸‹æ–‡ï¼Œè‡ªåŠ¨ç®¡ç†æ¶ˆæ¯çª—å£å¤§å°
   */
  async prepareConversationContext(
    chatHistory: ChatMessage[],
    currentMessage: string,
    modelName: string,
    systemPrompt?: string
  ): Promise<{
    messages: BaseMessage[]
    contextInfo: {
      originalMessageCount: number
      finalMessageCount: number
      tokenStats: {
        currentTokens: number
        maxTokens: number
        utilizationRate: number
        status: 'optimal' | 'near_limit' | 'compressed'
      }
      compressionApplied: boolean
      removedCount: number
    }
  }> {
    log.info('ğŸ“Š [ConversationManager] å¼€å§‹å‡†å¤‡å¯¹è¯ä¸Šä¸‹æ–‡')
    
    // 1. è½¬æ¢æ¶ˆæ¯æ ¼å¼
    const baseMessages = this.convertToBaseMessages(chatHistory)
    const newHumanMessage = new HumanMessage(currentMessage)
    
    // 2. æ„é€ å®Œæ•´æ¶ˆæ¯åˆ—è¡¨
    const systemMessage = systemPrompt ? new SystemMessage(systemPrompt) : null
    const allMessages: BaseMessage[] = [
      ...(systemMessage ? [systemMessage] : []),
      ...baseMessages,
      newHumanMessage
    ]

    // 3. è·å–æ¨¡å‹ä¸Šä¸‹æ–‡é™åˆ¶
    const maxTokens = this.getModelContextLimit(modelName)
    const targetTokens = Math.floor(maxTokens * 0.8) // ä½¿ç”¨80%çš„çª—å£ï¼Œç•™å‡ºç”Ÿæˆç©ºé—´

    log.info(`ğŸ“Š [ConversationManager] æ¨¡å‹: ${modelName}, æœ€å¤§tokens: ${maxTokens}, ç›®æ ‡tokens: ${targetTokens}`)

    // 4. æ£€æŸ¥æ˜¯å¦éœ€è¦è£å‰ª
    const originalTokens = this.estimateTokenCount(allMessages)
    const originalCount = allMessages.length
    
    let finalMessages = allMessages
    let compressionApplied = false
    let removedCount = 0

    if (originalTokens > targetTokens) {
      log.warn(`âš ï¸ [ConversationManager] Tokenè¶…é™: ${originalTokens}/${targetTokens}, å¼€å§‹è£å‰ªæ¶ˆæ¯`)
      
      // ä½¿ç”¨LangChainçš„trimMessagesè¿›è¡Œæ™ºèƒ½è£å‰ª
      finalMessages = await trimMessages(allMessages, {
        tokenCounter: (msgs) => this.estimateTokenCount(msgs),
        maxTokens: targetTokens,
        strategy: 'last', // ä¿ç•™æœ€æ–°çš„æ¶ˆæ¯
        startOn: 'human', // ç¡®ä¿ä»¥äººç±»æ¶ˆæ¯å¼€å§‹
        includeSystem: true, // ä¿ç•™ç³»ç»Ÿæ¶ˆæ¯
        allowPartial: false
      })
      
      compressionApplied = true
      removedCount = originalCount - finalMessages.length
      
      log.info(`âœ‚ï¸ [ConversationManager] æ¶ˆæ¯è£å‰ªå®Œæˆ: ${originalCount} -> ${finalMessages.length}, ç§»é™¤${removedCount}æ¡`)
    }

    // 5. è®¡ç®—æœ€ç»ˆtokenç»Ÿè®¡
    const finalTokens = this.estimateTokenCount(finalMessages)
    const utilizationRate = finalTokens / maxTokens
    
    let status: 'optimal' | 'near_limit' | 'compressed' = 'optimal'
    if (compressionApplied) {
      status = 'compressed'
    } else if (utilizationRate > 0.7) {
      status = 'near_limit'
    }

    const contextInfo = {
      originalMessageCount: originalCount,
      finalMessageCount: finalMessages.length,
      tokenStats: {
        currentTokens: finalTokens,
        maxTokens,
        utilizationRate,
        status
      },
      compressionApplied,
      removedCount
    }

    log.info(`ğŸ“Š [ConversationManager] ä¸Šä¸‹æ–‡å‡†å¤‡å®Œæˆ:`, contextInfo)

    return {
      messages: finalMessages,
      contextInfo
    }
  }

  /**
   * ä¸ºæ¨¡å‹åˆ‡æ¢å‡†å¤‡ä¸Šä¸‹æ–‡
   * å½“ç”¨æˆ·åˆ‡æ¢æ¨¡å‹æ—¶ï¼Œéœ€è¦é‡æ–°è¯„ä¼°ä¸Šä¸‹æ–‡çª—å£
   */
  async prepareContextForModelSwitch(
    chatHistory: ChatMessage[],
    fromModel: string,
    toModel: string,
    systemPrompt?: string
  ): Promise<{
    messages: BaseMessage[]
    contextInfo: any
    switchInfo: {
      fromLimit: number
      toLimit: number
      expansionPossible: boolean
      compressionNeeded: boolean
    }
  }> {
    const fromLimit = this.getModelContextLimit(fromModel)
    const toLimit = this.getModelContextLimit(toModel)
    
    log.info(`ğŸ”„ [ConversationManager] æ¨¡å‹åˆ‡æ¢: ${fromModel} (${fromLimit}) -> ${toModel} (${toLimit})`)

    // ä½¿ç”¨æ–°æ¨¡å‹çš„é™åˆ¶é‡æ–°å‡†å¤‡ä¸Šä¸‹æ–‡
    const result = await this.prepareConversationContext(
      chatHistory,
      '', // ç©ºæ¶ˆæ¯ï¼Œåªæ˜¯ä¸ºäº†é‡æ–°è®¡ç®—ä¸Šä¸‹æ–‡
      toModel,
      systemPrompt
    )

    // ç§»é™¤æˆ‘ä»¬æ·»åŠ çš„ç©ºæ¶ˆæ¯
    const messages = result.messages.slice(0, -1)

    const switchInfo = {
      fromLimit,
      toLimit,
      expansionPossible: toLimit > fromLimit,
      compressionNeeded: toLimit < fromLimit
    }

    return {
      messages,
      contextInfo: result.contextInfo,
      switchInfo
    }
  }

  /**
   * è·å–ä¸Šä¸‹æ–‡å¥åº·çŠ¶æ€
   */
  getContextHealth(chatHistory: ChatMessage[], modelName: string): {
    status: 'healthy' | 'warning' | 'critical'
    messageCount: number
    estimatedTokens: number
    maxTokens: number
    utilizationRate: number
    recommendations: string[]
  } {
    const baseMessages = this.convertToBaseMessages(chatHistory)
    const estimatedTokens = this.estimateTokenCount(baseMessages)
    const maxTokens = this.getModelContextLimit(modelName)
    const utilizationRate = estimatedTokens / maxTokens

    let status: 'healthy' | 'warning' | 'critical' = 'healthy'
    const recommendations: string[] = []

    if (utilizationRate > 0.9) {
      status = 'critical'
      recommendations.push('ä¸Šä¸‹æ–‡å³å°†æ»¡è½½ï¼Œå»ºè®®æ¸…ç†æ—©æœŸæ¶ˆæ¯')
      recommendations.push('è€ƒè™‘åˆ‡æ¢åˆ°æ›´å¤§ä¸Šä¸‹æ–‡çª—å£çš„æ¨¡å‹')
    } else if (utilizationRate > 0.7) {
      status = 'warning'
      recommendations.push('ä¸Šä¸‹æ–‡ä½¿ç”¨ç‡è¾ƒé«˜ï¼Œæ³¨æ„ç›‘æ§')
      recommendations.push('å¯è€ƒè™‘æ€»ç»“æ—©æœŸå¯¹è¯å†…å®¹')
    } else {
      recommendations.push('ä¸Šä¸‹æ–‡ä½¿ç”¨ç‡è‰¯å¥½')
    }

    return {
      status,
      messageCount: baseMessages.length,
      estimatedTokens,
      maxTokens,
      utilizationRate,
      recommendations
    }
  }
}

export const conversationManager = ConversationManager.getInstance()